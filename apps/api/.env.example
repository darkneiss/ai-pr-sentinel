PORT=3000
APP_VERSION=1.0.0
NODE_ENV=development

# GitHub
GITHUB_TOKEN=your_github_token
GITHUB_BOT_LOGIN=ai-pr-sentinel[bot]
# Shared secret used to verify GitHub webhook signatures (optional in dev).
GITHUB_WEBHOOK_SECRET=
# Optional override: true | false
# GITHUB_WEBHOOK_VERIFY_SIGNATURE=

# Logging: debug | info | warn | error
LOG_LEVEL=debug
LLM_LOG_RAW_RESPONSE=false

# AI triage toggle
AI_TRIAGE_ENABLED=true

# Optional mapping: AI semantic kind -> GitHub labels used by your repository.
# Fallbacks when unset:
# - AI_LABEL_KIND_BUG -> kind/bug
# - AI_LABEL_KIND_FEATURE -> kind/feature
# - AI_LABEL_KIND_QUESTION -> kind/question
# Example for GitHub default taxonomy:
# AI_LABEL_KIND_BUG=bug
# AI_LABEL_KIND_FEATURE=enhancement
# AI_LABEL_KIND_QUESTION=question

# Global timeout override (milliseconds). Applies to all providers if set.
LLM_TIMEOUT=

# --- LangSmith Observability (optional) ---
# LANGSMITH_TRACING=true
# LANGSMITH_API_KEY=your_langsmith_api_key
# LANGSMITH_PROJECT=ai-pr-sentinel
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com
# LANGSMITH_WORKSPACE_ID= # Optional, required for multi-workspace keys

# Provider selection: gemini | groq | ollama
# LLM_PROVIDER=gemini

# --- Gemini ---
# LLM_PROVIDER=gemini
# LLM_API_KEY=your_gemini_api_key
# LLM_MODEL=gemini-2.5-flash-lite
# LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta
# Note: if LLM_BASE_URL already contains "/models/...", it will be used as-is.
# The adapter adds "/models/${LLM_MODEL}:generateContent" when missing.
# The API key is sent via the "x-goog-api-key" header.

# --- Groq (OpenAI-compatible) ---
# LLM_PROVIDER=groq
# LLM_API_KEY=your_groq_api_key
# LLM_MODEL=openai/gpt-oss-20b
# LLM_BASE_URL=https://api.groq.com/openai/v1/chat/completions
# Note: Groq uses the URL exactly as provided (no path is appended).

# --- Ollama ---
# LLM_PROVIDER=ollama
# LLM_API_KEY=
# LLM_MODEL=llama3.1
# LLM_BASE_URL=http://127.0.0.1:11434/api/generate
# Note: Ollama uses the URL exactly as provided (no path is appended).
