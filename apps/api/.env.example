PORT=3000
APP_VERSION=1.0.0
GITHUB_TOKEN=your-github-token
GITHUB_BOT_LOGIN=ai-pr-sentinel[bot]
# Shared secret used to verify GitHub webhook signatures (must match GitHub Webhook "Secret").
# Behavior:
# - production: required (app fails fast if missing)
# - development: optional; if set, signature verification is enabled
GITHUB_WEBHOOK_SECRET=
# Optional override for advanced scenarios: true | false
# GITHUB_WEBHOOK_VERIFY_SIGNATURE=
NODE_ENV=development

# Logging: debug | info | warn | error
# Default: debug in non-production, info in production.
LOG_LEVEL=debug

# AI triage toggle
AI_TRIAGE_ENABLED=false

# Provider selection: gemini | ollama | groq
LLM_PROVIDER=ollama
# Global timeout override (milliseconds). Applies to all providers if set.
# Example: 300000 for 5 minutes.
LLM_TIMEOUT=
# Debug only: log truncated raw LLM responses when parsing fails.
# Disabled by default. Ignored in production.
LLM_LOG_RAW_RESPONSE=false

# Generic LLM settings (applies to selected provider)
# Notes:
# - For chat-completions compatible providers (e.g. Groq), the adapter calls:
#   {LLM_BASE_URL}/v1/chat/completions
# - Do NOT include "/v1/chat/completions" in LLM_BASE_URL.
# - Groq example:
#   LLM_PROVIDER=groq
#   LLM_BASE_URL=https://api.groq.com/openai
#   LLM_MODEL=llama-3.3-70b-versatile
LLM_API_KEY=
LLM_MODEL=
LLM_BASE_URL=

# Provider-specific fallback values (used only if LLM_* is empty)
GEMINI_API_KEY=
GROQ_API_KEY=
# OLLAMA_BASE_URL=http://127.0.0.1:11434
# OLLAMA_MODEL=llama3.1

# Optional provider-specific model/base URL overrides
# Gemini
# GEMINI_MODEL=gemini-1.5-flash
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
# Groq
# GROQ_MODEL=llama-3.3-70b-versatile
# GROQ_BASE_URL=https://api.groq.com/openai/v1
